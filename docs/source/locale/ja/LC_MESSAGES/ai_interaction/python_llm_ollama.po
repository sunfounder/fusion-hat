# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2026, SunFounder
# This file is distributed under the same license as the SunFounder Fusion
# HAT+ package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: SunFounder Fusion HAT+ \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-08 09:43+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: ja\n"
"Language-Team: ja <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.15.0\n"

#: ../ai_interaction/python_llm_ollama.rst:3
msgid ""
"Hello, welcome to the SunFounder Raspberry Pi & Arduino & ESP32 "
"Enthusiasts Community on Facebook! Dive deeper into Raspberry Pi, "
"Arduino, and ESP32 with fellow enthusiasts."
msgstr ""
"ã“ã‚“ã«ã¡ã¯ã€‚SunFounder Raspberry Pi & Arduino & ESP32 "
"Facebook æ„›å¥½å®¶ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã‚ˆã†ã“ãï¼ "
"Raspberry Piã€Arduinoã€ESP32 ã«ã¤ã„ã¦ã€ä»²é–“ã®æ„›å¥½å®¶ã¨ä¸€ç·’ã«ã•ã‚‰ã«æ·±ãæ¢æ±‚ã—ã¾ã—ã‚‡ã†ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:5
msgid "**Why Join?**"
msgstr "**å‚åŠ ã™ã‚‹ç†ç”±** "

#: ../ai_interaction/python_llm_ollama.rst:7
msgid ""
"**Expert Support**: Solve post-sale issues and technical challenges with "
"help from our community and team."
msgstr ""
"**å°‚é–€çš„ãªã‚µãƒãƒ¼ãƒˆ** ï¼šã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ¡ãƒ³ãƒãƒ¼ã‚„å…¬å¼ãƒãƒ¼ãƒ ã®æ”¯æ´ã‚’å—ã‘ã¦ã€"
"è³¼å…¥å¾Œã®å•é¡Œã‚„æŠ€è¡“çš„ãªèª²é¡Œã‚’è§£æ±ºã§ãã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:8
msgid "**Learn & Share**: Exchange tips and tutorials to enhance your skills."
msgstr "**å­¦ã³ã¨å…±æœ‰** ï¼šãƒ’ãƒ³ãƒˆã‚„ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’äº¤æ›ã—ã€ã‚¹ã‚­ãƒ«ã‚’å‘ä¸Šã•ã›ã¾ã—ã‚‡ã†ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:9
msgid ""
"**Exclusive Previews**: Get early access to new product announcements and"
" sneak peeks."
msgstr ""
"**é™å®šå…ˆè¡Œæƒ…å ±** ï¼šæ–°è£½å“ã®ç™ºè¡¨ã‚„ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ã‚’ã„ã¡æ—©ãå…¥æ‰‹ã§ãã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:10
msgid "**Special Discounts**: Enjoy exclusive discounts on our newest products."
msgstr "**ç‰¹åˆ¥å‰²å¼•** ï¼šæœ€æ–°è£½å“ã‚’å¯¾è±¡ã¨ã—ãŸé™å®šå‰²å¼•ã‚’ãŠæ¥½ã—ã¿ã„ãŸã ã‘ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:11
msgid ""
"**Festive Promotions and Giveaways**: Take part in giveaways and holiday "
"promotions."
msgstr ""
"**å­£ç¯€é™å®šãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã¨ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆä¼ç”»** ï¼š"
"ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚„ç¥æ—¥é™å®šã®ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã«å‚åŠ ã§ãã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:13
msgid ""
"ğŸ‘‰ Ready to explore and create with us? Click [|link_sf_facebook|] and "
"join today!"
msgstr ""
"ğŸ‘‰ ç§ãŸã¡ã¨ä¸€ç·’ã«æ¢æ±‚ã—ã€å‰µé€ ã™ã‚‹æº–å‚™ã¯ã§ãã¾ã—ãŸã‹ï¼Ÿ "
"[|link_sf_facebook|] ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ä»Šã™ãå‚åŠ ã—ã¾ã—ã‚‡ã†ï¼"

#: ../ai_interaction/python_llm_ollama.rst:16
msgid "4. Text Vision Talk with Ollama"
msgstr "4. Ollama ã§ Text / Vision / Talk"

#: ../ai_interaction/python_llm_ollama.rst:18
msgid ""
"In this lesson, you will learn how to use **Ollama**, a tool for running "
"large language and vision models locally. We will show you how to install"
" Ollama, download a model, and connect Fusion HAT+ to it."
msgstr ""
"ã“ã®ãƒ¬ãƒƒã‚¹ãƒ³ã§ã¯ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŠã‚ˆã³ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã§ãã‚‹ãƒ„ãƒ¼ãƒ«ã§ã‚ã‚‹ "
"**Ollama** ã®ä½¿ã„æ–¹ã‚’å­¦ã³ã¾ã™ã€‚"
"Ollama ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãã—ã¦ Fusion HAT+ ã‚’æ¥ç¶šã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:21
msgid ""
"With this setup, Fusion HAT+ can take a camera snapshot and the model "
"will **see and tell** â€” you can ask any question about the image, and the"
" model will reply in natural language."
msgstr ""
"ã“ã®è¨­å®šã«ã‚ˆã‚Šã€Fusion HAT+ ã¯ã‚«ãƒ¡ãƒ©ã§ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®å½±ã§ãã€ãƒ¢ãƒ‡ãƒ«ãŒ "
"**è¦‹ã¦èª¬æ˜** ã—ã¾ã™ã€‚"
"ç”»åƒã«ã¤ã„ã¦ã©ã‚“ãªè³ªå•ã§ã‚‚ã§ãã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªç„¶è¨€èªã§è¿”ä¿¡ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:27
msgid "1. Install Ollama (LLM) and Download Model"
msgstr "1. Ollamaï¼ˆ LLM ï¼‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"

#: ../ai_interaction/python_llm_ollama.rst:29
msgid "You can choose where to install **Ollama**:"
msgstr "**Ollama** ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å ´æ‰€ã‚’é¸ã¹ã¾ã™ï¼š"

#: ../ai_interaction/python_llm_ollama.rst:31
msgid "On your Raspberry Pi (local run)"
msgstr "Raspberry Pi ä¸Šï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼‰"

#: ../ai_interaction/python_llm_ollama.rst:32
msgid "Or on another computer (Mac/Windows/Linux) in the **same local network**"
msgstr "ã¾ãŸã¯ **åŒä¸€ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯** å†…ã®åˆ¥ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ï¼ˆ Mac / Windows / Linux ï¼‰"

#: ../ai_interaction/python_llm_ollama.rst:34
msgid "**Recommended models vs hardware**"
msgstr "**æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®ç›®å®‰** "

#: ../ai_interaction/python_llm_ollama.rst:36
msgid ""
"You can choose any model available on |link_ollama_hub|. Models come in "
"different sizes (3B, 7B, 13B, 70B...). Smaller models run faster and "
"require less memory, while larger models provide better quality but need "
"powerful hardware."
msgstr ""
"|link_ollama_hub| ã§æä¾›ã•ã‚Œã¦ã„ã‚‹ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã§ãã¾ã™ã€‚"
"ãƒ¢ãƒ‡ãƒ«ã«ã¯ã•ã¾ã–ã¾ãªã‚µã‚¤ã‚ºï¼ˆ 3Bã€7Bã€13Bã€70B... ï¼‰ãŒã‚ã‚Šã€"
"å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã»ã©é«˜é€Ÿã«å‹•ä½œã—ã€å¿…è¦ãªãƒ¡ãƒ¢ãƒªã‚‚å°‘ãªããªã‚Šã¾ã™ã€‚"
"ä¸€æ–¹ã§ã€å¤§ãã„ãƒ¢ãƒ‡ãƒ«ã¯å“è³ªãŒå‘ä¸Šã—ã¾ã™ãŒã€ã‚ˆã‚Šå¼·åŠ›ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãŒå¿…è¦ã§ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:40
msgid "Check the table below to decide which model size fits your device."
msgstr "ä»¥ä¸‹ã®è¡¨ã‚’ç¢ºèªã—ã¦ã€ãƒ‡ãƒã‚¤ã‚¹ã«åˆã†ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’é¸ã‚“ã§ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:46
msgid "Model size"
msgstr "ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º"

#: ../ai_interaction/python_llm_ollama.rst:47
msgid "Min RAM Required"
msgstr "å¿…è¦æœ€å° RAM"

#: ../ai_interaction/python_llm_ollama.rst:48
msgid "Recommended Hardware"
msgstr "æ¨å¥¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢"

#: ../ai_interaction/python_llm_ollama.rst:49
msgid "~3B parameters"
msgstr "ç´„ 3B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"

#: ../ai_interaction/python_llm_ollama.rst:50
msgid "8GB (16GB better)"
msgstr "8GBï¼ˆ 16GB æ¨å¥¨ ï¼‰"

#: ../ai_interaction/python_llm_ollama.rst:51
msgid "Raspberry Pi 5 (16GB) or mid-range PC/Mac"
msgstr "Raspberry Pi 5ï¼ˆ 16GB ï¼‰ã¾ãŸã¯ãƒŸãƒ‰ãƒ«ãƒ¬ãƒ³ã‚¸ã® PC / Mac"

#: ../ai_interaction/python_llm_ollama.rst:52
msgid "~7B parameters"
msgstr "ç´„ 7B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"

#: ../ai_interaction/python_llm_ollama.rst:53
msgid "16GB+"
msgstr "16GB ä»¥ä¸Š"

#: ../ai_interaction/python_llm_ollama.rst:54
msgid "Pi 5 (16GB, just usable) or mid-range PC/Mac"
msgstr "Pi 5ï¼ˆ 16GBã€ã‚®ãƒªã‚®ãƒªå‹•ä½œ ï¼‰ã¾ãŸã¯ãƒŸãƒ‰ãƒ«ãƒ¬ãƒ³ã‚¸ã® PC / Mac"

#: ../ai_interaction/python_llm_ollama.rst:55
msgid "~13B parameters"
msgstr "ç´„ 13B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"

#: ../ai_interaction/python_llm_ollama.rst:56
msgid "32GB+"
msgstr "32GB ä»¥ä¸Š"

#: ../ai_interaction/python_llm_ollama.rst:57
msgid "Desktop PC / Mac with high RAM"
msgstr "å¤§å®¹é‡ RAM ã‚’æ­è¼‰ã—ãŸãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ— PC / Mac"

#: ../ai_interaction/python_llm_ollama.rst:58
msgid "30B+ parameters"
msgstr "30B ä»¥ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"

#: ../ai_interaction/python_llm_ollama.rst:59
msgid "64GB+"
msgstr "64GB ä»¥ä¸Š"

#: ../ai_interaction/python_llm_ollama.rst:60
msgid "Workstation / Server / GPU recommended"
msgstr "ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ã‚µãƒ¼ãƒãƒ¼ï¼ˆ GPU æ¨å¥¨ ï¼‰"

#: ../ai_interaction/python_llm_ollama.rst:61
msgid "70B+ parameters"
msgstr "70B ä»¥ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"

#: ../ai_interaction/python_llm_ollama.rst:62
msgid "128GB+"
msgstr "128GB ä»¥ä¸Š"

#: ../ai_interaction/python_llm_ollama.rst:63
msgid "High-end server with multiple GPUs"
msgstr "è¤‡æ•° GPU ã‚’æ­è¼‰ã—ãŸãƒã‚¤ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼"

#: ../ai_interaction/python_llm_ollama.rst:65
msgid "**Install on Raspberry Pi**"
msgstr "**Raspberry Pi ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«** "

#: ../ai_interaction/python_llm_ollama.rst:67
msgid "If you want to run Ollama directly on your Raspberry Pi:"
msgstr "Raspberry Pi ã§ Ollama ã‚’ç›´æ¥å‹•ã‹ã—ãŸã„å ´åˆï¼š"

#: ../ai_interaction/python_llm_ollama.rst:69
msgid "Use a **64-bit Raspberry Pi OS**"
msgstr "**64 ãƒ“ãƒƒãƒˆç‰ˆ Raspberry Pi OS** ã‚’ä½¿ç”¨ã™ã‚‹"

#: ../ai_interaction/python_llm_ollama.rst:70
msgid "Strongly recommended: **Raspberry Pi 5 (16GB RAM)**"
msgstr "å¼·ãæ¨å¥¨ï¼š **Raspberry Pi 5ï¼ˆ 16GB RAM ï¼‰** "

#: ../ai_interaction/python_llm_ollama.rst:72
msgid "Run the following commands:"
msgstr "ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š"

#: ../ai_interaction/python_llm_ollama.rst:89
msgid "**Install on Mac / Windows / Linux (Desktop App)**"
msgstr "**Mac / Windows / Linuxï¼ˆãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªï¼‰ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«** "

#: ../ai_interaction/python_llm_ollama.rst:91
msgid "Download and install Ollama from |link_ollama|"
msgstr "|link_ollama| ã‹ã‚‰ Ollama ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:95
msgid ""
"Open the Ollama app, go to the **Model Selector**, and use the search bar"
" to find a model. For example, type ``llama3.2:3b`` (a small and "
"lightweight model to start with)."
msgstr ""
"Ollama ã‚¢ãƒ—ãƒªã‚’é–‹ãã€ **Model Selector** ã«ç§»å‹•ã—ã¦æ¤œç´¢ãƒãƒ¼ã§ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã—ã¾ã™ã€‚"
"ä¾‹ãˆã°ã€ ``llama3.2:3b`` ã¨å…¥åŠ›ã—ã¾ã™ï¼ˆå°ã•ãè»½é‡ã§ã€æœ€åˆã«è©¦ã™ã®ã«é©ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ï¼‰ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:99
msgid ""
"After the download is complete, type something simple like â€œHiâ€ in the "
"chat window, Ollama will automatically start downloading it when you "
"first use it."
msgstr ""
"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ãŸã‚‰ã€ãƒãƒ£ãƒƒãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ã€ŒHiã€ãªã©ã®ç°¡å˜ãªæ–‡ç« ã‚’å…¥åŠ›ã—ã¦ã¿ã¦ãã ã•ã„ã€‚"
"åˆå›åˆ©ç”¨æ™‚ã«ã¯ã€Ollama ãŒè‡ªå‹•çš„ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:103
msgid ""
"Go to **Settings** â†’ enable **Expose Ollama to the network**. This allows"
" your Raspberry Pi to connect to it over LAN."
msgstr ""
" **Settings** â†’ **Expose Ollama to the network** ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚"
"ã“ã‚Œã«ã‚ˆã‚Šã€Raspberry Pi ãŒ LAN çµŒç”±ã§ Ollama ã«æ¥ç¶šã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:109
msgid "If you see an error like:"
msgstr "æ¬¡ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã‚‹å ´åˆï¼š"

#: ../ai_interaction/python_llm_ollama.rst:111
msgid "``Error: model requires more system memory ...``"
msgstr "``Error: model requires more system memory ...`` "

#: ../ai_interaction/python_llm_ollama.rst:113
msgid ""
"The model is too large for your machine. Use a **smaller model** or "
"switch to a computer with more RAM."
msgstr ""
"ãƒ¢ãƒ‡ãƒ«ãŒãƒã‚·ãƒ³ã«å¯¾ã—ã¦å¤§ãã™ãã¾ã™ã€‚"
"**ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«** ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€RAM ã®å¤šã„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã«åˆ‡ã‚Šæ›¿ãˆã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:117
msgid "2. Test Ollama"
msgstr "2. Ollama ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹"

#: ../ai_interaction/python_llm_ollama.rst:119
msgid ""
"Once Ollama is installed and your model is ready, you can quickly test it"
" with a minimal chat loop."
msgstr ""
"Ollama ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒã§ããŸã‚‰ã€"
"æœ€å°é™ã®ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—ã§ç´ æ—©ãå‹•ä½œç¢ºèªã§ãã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:121
msgid "**Run the program**"
msgstr "**ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’å®Ÿè¡Œ** "

#: ../ai_interaction/python_llm_ollama.rst:128
msgid "Now you can chat with Fusion HAT+ directly from the terminal."
msgstr "ã“ã‚Œã§ã€ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ Fusion HAT+ ã¨ç›´æ¥ãƒãƒ£ãƒƒãƒˆã§ãã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:130
msgid ""
"You can choose **any model** available on |link_ollama_hub|, but smaller "
"models (e.g. ``moondream:1.8b``, ``phi3:mini``) are recommended if you "
"only have 8â€“16GB RAM."
msgstr ""
"|link_ollama_hub| ã§æä¾›ã•ã‚Œã¦ã„ã‚‹ **ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«** ã‚’é¸ã¹ã¾ã™ãŒã€"
"RAM ãŒ 8ï½16GB ã—ã‹ãªã„å ´åˆã¯ã€"
"å°ã•ã„ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹ï¼š ``moondream:1.8b`` ã€ ``phi3:mini`` ï¼‰ãŒãŠã™ã™ã‚ã§ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:131
msgid ""
"Make sure the model you specify in the code matches the model you have "
"already pulled in Ollama."
msgstr ""
"ã‚³ãƒ¼ãƒ‰ã§æŒ‡å®šã™ã‚‹ãƒ¢ãƒ‡ãƒ«åãŒã€Ollama ã§ã™ã§ã« pull æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:132
msgid "Type ``exit`` or ``quit`` to stop the program."
msgstr "``exit`` ã¾ãŸã¯ ``quit`` ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:133
msgid ""
"If you cannot connect, ensure that Ollama is running and that both "
"devices are on the same LAN if you are using a remote host."
msgstr ""
"æ¥ç¶šã§ããªã„å ´åˆã¯ã€Ollama ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
"ãƒªãƒ¢ãƒ¼ãƒˆãƒ›ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã¯ã€ä¸¡æ–¹ã®ãƒ‡ãƒã‚¤ã‚¹ãŒåŒã˜ LAN ä¸Šã«ã‚ã‚‹ã“ã¨ã‚‚ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:135
#: ../ai_interaction/python_llm_ollama.rst:211
msgid "**Code**"
msgstr "**ã‚³ãƒ¼ãƒ‰** "

#: ../ai_interaction/python_llm_ollama.rst:175
msgid "3. Vision Talk with Ollama"
msgstr "3. Ollama ã§ Vision Talk"

#: ../ai_interaction/python_llm_ollama.rst:177
msgid ""
"In this demo, the Pi camera takes a snapshot **each time you type a "
"question**. The program sends **your typed text + the new photo** to a "
"local vision model via Ollama, and then streams the modelâ€™s reply in "
"plain English. This is a minimal â€œsee & tellâ€ baseline you can later "
"extend with color/face/QR checks."
msgstr ""
"ã“ã®ãƒ‡ãƒ¢ã§ã¯ã€è³ªå•ã‚’ **å…¥åŠ›ã™ã‚‹ãŸã³ã«** Pi ã‚«ãƒ¡ãƒ©ãŒã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®å½±ã—ã¾ã™ã€‚"
"ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ã€ **å…¥åŠ›ã—ãŸãƒ†ã‚­ã‚¹ãƒˆ + æ–°ã—ã„å†™çœŸ** ã‚’ Ollama çµŒç”±ã§ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã«é€ä¿¡ã—ã€"
"ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’å¹³æ˜“ãªè‹±èªã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤ºã—ã¾ã™ã€‚"
"ã“ã‚Œã¯æœ€å°é™ã®ã€Œ see & tell ã€ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§ã€å¾Œã‹ã‚‰è‰²ï¼é¡”ï¼QR ã®ãƒã‚§ãƒƒã‚¯ãªã©ã«æ‹¡å¼µã§ãã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:182
msgid "**Before You Start**"
msgstr "**å§‹ã‚ã‚‹å‰ã«** "

#: ../ai_interaction/python_llm_ollama.rst:184
msgid ""
"Open the **Ollama** app (or run the service) and make sure a **vision-"
"capable model** is pulled."
msgstr ""
" **Ollama** ã‚¢ãƒ—ãƒªã‚’é–‹ãï¼ˆã¾ãŸã¯ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•ã™ã‚‹ï¼‰ã—ã€"
"**ãƒ“ã‚¸ãƒ§ãƒ³å¯¾å¿œãƒ¢ãƒ‡ãƒ«** ãŒ pull æ¸ˆã¿ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:186
msgid "If you have enough memory (â‰¥16GB RAM), you may try ``llava:7b``."
msgstr "ååˆ†ãªãƒ¡ãƒ¢ãƒªï¼ˆ 16GB RAM ä»¥ä¸Šï¼‰ãŒã‚ã‚‹å ´åˆã¯ã€ ``llava:7b`` ã‚’è©¦ã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:187
msgid ""
"If you only have **8GB RAM**, prefer a smaller model such as "
"``moondream:1.8b`` or ``granite3.2-vision:2b``."
msgstr ""
"RAM ãŒ **8GB** ã®ã¿ã®å ´åˆã¯ã€"
"``moondream:1.8b`` ã¾ãŸã¯ ``granite3.2-vision:2b`` ã®ã‚ˆã†ãªå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:191
msgid "**Run the Demo**"
msgstr "**ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ** "

#: ../ai_interaction/python_llm_ollama.rst:193
msgid "Go to the example folder and run the script:"
msgstr "ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•ã—ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ï¼š"

#: ../ai_interaction/python_llm_ollama.rst:200
msgid "What happens when it runs:"
msgstr "å®Ÿè¡Œæ™‚ã®å‹•ä½œï¼š"

#: ../ai_interaction/python_llm_ollama.rst:202
msgid "The program prints a welcome line and waits for your input (``>>>``)."
msgstr "ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã—ã€å…¥åŠ›ï¼ˆ ``>>>`` ï¼‰ã‚’å¾…ã¡ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:203
msgid ""
"**Every time you type anything** (e.g., â€œhelloâ€, â€œIs there yellow?â€, â€œAny"
" faces?â€, â€œWhat is on the desk?â€), it:"
msgstr ""
"**ä½•ã‹ã‚’å…¥åŠ›ã™ã‚‹ãŸã³ã«** ï¼ˆä¾‹ï¼šã€Œhelloã€ã€ã€ŒIs there yellow?ã€ã€ã€ŒAny faces?ã€ã€ã€ŒWhat is on the desk?ã€ï¼‰"
"æ¬¡ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š"

#: ../ai_interaction/python_llm_ollama.rst:205
msgid "**captures a photo** from the Pi camera (saved to ``/tmp/llm-img.jpg``),"
msgstr "Pi ã‚«ãƒ¡ãƒ©ã‹ã‚‰ **å†™çœŸã‚’æ’®å½±** ã—ï¼ˆ ``/tmp/llm-img.jpg`` ã«ä¿å­˜ï¼‰ã€"

#: ../ai_interaction/python_llm_ollama.rst:206
msgid "**sends your text + the photo** to the vision model via Ollama,"
msgstr "Ollama çµŒç”±ã§ **ãƒ†ã‚­ã‚¹ãƒˆ + å†™çœŸ** ã‚’ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¸é€ä¿¡ã—ã€"

#: ../ai_interaction/python_llm_ollama.rst:207
msgid "**streams back** the modelâ€™s answer to the terminal."
msgstr "ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã¸ **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º** ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:209
msgid "Type ``exit`` or ``quit`` to end the program."
msgstr "``exit`` ã¾ãŸã¯ ``quit`` ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:271
msgid "Troubleshooting"
msgstr "ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"

#: ../ai_interaction/python_llm_ollama.rst:274
msgid "**I get an error like: `model requires more system memory ...`.**"
msgstr "**`model requires more system memory ...` ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã™ã€‚** "

#: ../ai_interaction/python_llm_ollama.rst:276
msgid "This means the model is too large for your device."
msgstr "ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ‡ãƒã‚¤ã‚¹ã«å¯¾ã—ã¦å¤§ãã™ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:277
msgid ""
"Use a smaller model such as ``moondream:1.8b`` or "
"``granite3.2-vision:2b``."
msgstr ""
"``moondream:1.8b`` ã¾ãŸã¯ ``granite3.2-vision:2b`` ã®ã‚ˆã†ãªå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:278
msgid "Or switch to a machine with more RAM and expose Ollama to the network."
msgstr "ã¾ãŸã¯ã€ã‚ˆã‚Šå¤šãã® RAM ã‚’æ­è¼‰ã—ãŸãƒã‚·ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆã€Ollama ã‚’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å…¬é–‹ã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:280
msgid "**The code cannot connect to Ollama (connection refused).**"
msgstr "**ã‚³ãƒ¼ãƒ‰ãŒ Ollama ã«æ¥ç¶šã§ãã¾ã›ã‚“ï¼ˆ connection refused ï¼‰ã€‚** "

#: ../ai_interaction/python_llm_ollama.rst:282
msgid "Check the following:"
msgstr "ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š"

#: ../ai_interaction/python_llm_ollama.rst:284
msgid "Make sure Ollama is running (``ollama serve`` or the desktop app is open)."
msgstr "Ollama ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆ ``ollama serve`` ã¾ãŸã¯ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã‚¢ãƒ—ãƒªãŒé–‹ã„ã¦ã„ã‚‹ã“ã¨ï¼‰ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:285
msgid ""
"If using a remote computer, enable **Expose to network** in Ollama "
"settings."
msgstr ""
"ãƒªãƒ¢ãƒ¼ãƒˆã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€Ollama ã®è¨­å®šã§ **Expose to network** ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:286
msgid ""
"Double-check that the ``ip=\"...\"`` in your code matches the correct LAN"
" IP."
msgstr ""
"ã‚³ãƒ¼ãƒ‰å†…ã® ``ip=\"...\"`` ãŒæ­£ã—ã„ LAN ã® IP ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹å†ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:287
msgid "Confirm both devices are on the same local network."
msgstr "ä¸¡æ–¹ã®ãƒ‡ãƒã‚¤ã‚¹ãŒåŒã˜ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸Šã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:289
msgid "**My Pi camera does not capture anything.**"
msgstr "**Pi ã‚«ãƒ¡ãƒ©ã§ä½•ã‚‚æ’®å½±ã§ãã¾ã›ã‚“ã€‚** "

#: ../ai_interaction/python_llm_ollama.rst:291
msgid ""
"Verify that ``Picamera2`` is installed and working with a simple test "
"script."
msgstr ""
"``Picamera2`` ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ãŠã‚Šã€ç°¡å˜ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:292
msgid ""
"Check that the camera cable is properly connected and enabled in ``raspi-"
"config``."
msgstr ""
"ã‚«ãƒ¡ãƒ©ã‚±ãƒ¼ãƒ–ãƒ«ãŒæ­£ã—ãæ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€ãã—ã¦ ``raspi-config`` ã§æœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:293
msgid ""
"Ensure your script has permission to write to the target path (``/tmp"
"/llm-img.jpg``)."
msgstr ""
"ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒä¿å­˜å…ˆãƒ‘ã‚¹ï¼ˆ ``/tmp/llm-img.jpg`` ï¼‰ã«æ›¸ãè¾¼ã‚€æ¨©é™ã‚’æŒã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:295
msgid "**The output is too slow.**"
msgstr "**å‡ºåŠ›ãŒé…ã™ãã¾ã™ã€‚** "

#: ../ai_interaction/python_llm_ollama.rst:297
msgid "Smaller models reply faster, but with simpler answers."
msgstr "å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã»ã©å¿œç­”ã¯é€Ÿããªã‚Šã¾ã™ãŒã€å›ç­”ã¯ã‚ˆã‚Šç°¡æ½”ã«ãªã‚Šã¾ã™ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:298
msgid ""
"You can lower the camera resolution (e.g., 640Ã—480 instead of 1280Ã—720) "
"to speed up image processing."
msgstr ""
"ç”»åƒå‡¦ç†ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€ã‚«ãƒ¡ãƒ©è§£åƒåº¦ã‚’ä¸‹ã’ã‚‹ã“ã¨ãŒã§ãã¾ã™"
"ï¼ˆä¾‹ï¼š 1280Ã—720 ã§ã¯ãªã 640Ã—480 ï¼‰ã€‚"

#: ../ai_interaction/python_llm_ollama.rst:299
msgid "Close other programs on your Pi to free up CPU and RAM."
msgstr "Pi ä¸Šã§ä»–ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’é–‰ã˜ã¦ã€CPU ã¨ RAM ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚"
